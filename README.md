# Reservoir-Computing-in-Phone-Recognition
This study explores the potential of Reservoir Computing models (RCs) as large- vocabulary, continuous speech, speaker-dependent acoustic model in phone recognition.

# Abstract
RCs emerged in the early 2000s as a unifying framework for Recurrent Neural Network models (RNNs) framework. It offers faster learning convergence with compact model sizes and fast training e<iciency due to its structure of a fixed, dynamic reservoir and a light, trainable readout. In this paper, with a comparison to Long-Short-Term- Memory models (LSTMs), we examine RCs performance and e<iciency in terms of training data size and parameters size. The results align with our central hypothesis that RCs are more compact and e<icient than LSTMs, giving the model a boast in catering to scenarios where practitioners face constrains in both data availability and computational resources. We also explore Hierarchal Reservoir Computing models (HRCs), a multilayer- RCs, in phone recognition, and, contrary to our hypothesis, increasing the layer number does not induce a significant improvement than the increase in the reservoir size. Lastly, we examine how far RCs are from state-of-the-art of acoustic model, Wav2Vec 2.0 in terms of performances, and RCs’ advantages in training data sizes and parameter sizes. This research provides a solid example for future work related to RCs and HRCs in speech recognition, particularly the architecture variation and hyperparameter optimisation in HRCs and Physical Reservoir Computing (PRC), which has been found yielding comparable results to Transformers.

# Research Aim
In this project we aim to develop an ESN-based ASR system that uses large-vocabulary, continuous speech and speaker-dependent model. This means we do not take di<erent dialects and speaker styles into account. We do not consider the robustness of the model in noises, nor the model ability in recognising interjections. The model is English language focused, and we solely focus on the performance in predicting the correct English labels from the clean, scripted audio data. Therefore, we use LibreSpeech dataset which are audiobooks recordings done in studios by one female speaker with an American accent.
The project aims to complete the first three modules in a standard ASR system, which is Speech Signal Acquisition, Feature Extraction, and Acoustic Modelling. The acoustic model predicts the phoneme from the audio signal, and we use PER (which is calculated in the same way as WER but treating phones as words) and confusion metrics as evaluation metrics.
To answer the four research questions (listed in Section 1), the models we choose to compare in the experiments are single ESNs, HRC, LSTMs, bi-LSTMs, and Wav2Vec2. One approach with HRC would be to work with longer sub-word units in the higher layers (e.g., phones in the 1st layer and syllables in the 2nd layer), but in this work, we stick to working solely on phones.
There are 4 experiments in this project:
1. In the first experiment, we compare the performance of single ESNs and single LSTMs to a function of training data size. We examine how ESNs perform in terms of data size e8iciency. We also examine how the two models perform di8erently when both are models in RNNs framework.
2. In the second experiment, we compare the performance of single ESNs, LSTMs, and bi-LSTMs in phone recognition to a function of parameter size. We examine how ESNs perform in terms of parameter sizes (tuned by reservoir sizes), and whether they exceed LSTMs when two model types have similar parameter sizes.
3. In the third experiment, we explore multi-reservoir ESNs using a hierarchal architecture, the HRC. We explore their performance in phone recognition when multiple reservoirs in di8erent sizes are used.
4. In the fourth experiment, from the ESNs used in the first three experiments we pick those that had the best results and compare them with an existing work using Wav2Vec2 in phone recognition. This gives us an understanding of how ESNs perform comparing to the state-of- the-art of ASR models. In the Discussion section, we delve deep into other RC- based models and explore its potential of achieving the same results as the state- of-the-art of ASR models.

# About Dataset
This project uses LibriSpeech dataset (Panayotov et al., 2015) for model training and testing. LibriSpeech is an ASR corpus containing 1000 hours of read English speech derived from audiobooks made in Project Gutenberg (Gerlach & Font-Clos, 2020). The 1000 hours of speech is sampled at 16 kHz, including three train subsets: 100 hours (train- clean-100), 360 hours (train-clean-360) and 500 hours (train-other-500). We use the train-clean- 360 data with a 4:1 of train/test split. Audio data is provided in .flac format. Each audio file is approximately 15 seconds long.
Every audio file has a corresponding .TextGrid file that provides the annotations of the audio. The annotation includes the absolute timestamps and phonetic labels, generated from Montreal Forced Aligner. The alignments suggest how each phonetic label delimits the audio. The annotation contains data fields that include, firstly, “text” which is the type of phone spoken, secondly, “xmin” which is the start time of the phone spoken in seconds, and lastly, “xmax” which is the end time of the phone spoken in seconds.
Typically, the conventional agreement is that the phonological system of the English language is composed of 44 phonemes, of which 24 are consonants and 20 are vowels (Bizzocchi, 2017). However, employing different definitions of phoneme, phone, and allophone can result 39 to 44 phonemes in English phoneme inventories. The LibriSpeech annotations includes 71 phonemes deriving from 39 English phonemes with stress marking (e.g. there are AE0, AE1, AE2 which all suggest the sound of phone AE with different stress).
The dataset was selected due to its appropriate size, consistent style, and comprehensive annotations. For this project, we are not considering noises or varieties of speaking style, and we solely focus on one language – English; therefore, using an English audiobook recording done by a single female speaker reading from a script in a studio as training data is suitable for this project.

